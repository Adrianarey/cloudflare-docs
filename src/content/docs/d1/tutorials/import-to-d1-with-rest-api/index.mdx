---
updated: 2024-10-28
difficulty: Beginner
content_type: Tutorial
pcx_content_type: tutorial
title: Bulk import to D1 using REST API
products:
  - D1
languages:
  - JavaScript
  - TypeScript
  - SQL
---

import { Render, Details } from "~/components";

In this tutorial, you will learn how to import a database into D1 using REST API.

## Prerequisites

<Render file="prereqs" product="workers" />

## 1. Create a D1 API token

To use REST APIs, you need to generate an API token to authenticate your API requests. You can do this through the Cloudflare dashboard.

<Render file="generate-d1-api-token" product="d1" />

## 2. Create the target table

You must have an existing D1 table which matches the schema of the data you wish to import.

This tutorial uses example data:

- A database called `d1-import-tutorial`.
- A table called `TargetD1Table`
- Within `TargetD1Table`, three columns called `id`, `text`, and `date_added`.

1. Go to **Storage & Databases > D1**.
2. Select **Create**.
3. Name your database. For this tutorial, name your D1 database `d1-import-tutorial`.
4. (Optional) Provide a location hint. Location hint is an optional parameter you can provide to indicate your desired geographical location for your database. Refer to [Provide a location hint](/d1/configuration/data-location/#provide-a-location-hint) for more information.
5. Select **Create**.
6. Go to **Console**, then paste the following SQL snippet. This creates a table named `TargetD1Table`.

```sql
DROP TABLE IF EXISTS TargetD1Table;
CREATE TABLE IF NOT EXISTS TargetD1Table (id INTEGER PRIMARY KEY, text TEXT, date_added TEXT);
```

## 3. Create an `index` file

Create a new file called `index`. This file will contain the code which uses REST API to import your data to a D1 database.

Define your variables and set:
- `TARGET_TABLE`: The target table name
- `ACCOUNT_ID`: The account ID (you can find this in the Cloudflare dashboard > **Workers & Pages**)
- `DATABASE_ID`: The D1 database ID (you can find this in the Cloudflare dashboard > **Storage & Databases** > **D1 SQL Database** > your database)
- `D1_API_KEY`: The D1 API token generated in [step 1](#1-create-a-d1-api-token)

```js title="index"
const TARGET_TABLE = " "; // for the tutorial, `TargetD1Table`
const ACCOUNT_ID = " ";
const DATABASE_ID = " ";
const D1_API_KEY = ' ';
const D1_URL = `https://api.cloudflare.com/client/v4/accounts/${ACCOUNT_ID}/d1/database/${DATABASE_ID}/import`;
const filename = crypto.randomUUID();
const uploadSize = 2500;
const headers = {
  'Content-Type': 'application/json',
  'Authorization': `Bearer ${D1_API_KEY}`
};
```

## 4. Generate example data (optional)

In practice, you may already have the data you wish to import to a D1 database.

This tutorial generates example data to demonstrate the import process.

1. Install the `@faker-js/faker` module.

```sh
npm install @faker-js/faker
```

2. Add the following code at the beginning of the `index` file. This code creates an array called `data` with 10 array elements, where each array element contains an object with `id`, `text`, and `date_added`. Each array element corresponds to a table row.

```js
import crypto from 'crypto';
import { faker } from "@faker-js/faker";

// Generate Fake data
const data = Array.from({ length: 10 }, () => ({
  id: crypto.randomUUID(),
  text: faker.lorem.paragraph(),
  date_added: new Date().toISOString().slice(0, 19).replace("T", " ")
}));
```

## 5. Prepare the SQL command

Prepare the array into a SQL command.

```js
Placeholder
```

## 6. Write a function to import to D1

## 7. Final code

The final code of your `index` file should look as below.

```js
import crypto from 'crypto';
import { faker } from "@faker-js/faker";

const TARGET_TABLE = " ";
const ACCOUNT_ID = " ";
const DATABASE_ID = " ";
const D1_API_KEY = ' ';
const D1_URL = `https://api.cloudflare.com/client/v4/accounts/${ACCOUNT_ID}/d1/database/${DATABASE_ID}/import`;
const filename = crypto.randomUUID();
const uploadSize = 2500;
const headers = {
  'Content-Type': 'application/json',
  'Authorization': `Bearer ${D1_API_KEY}`
};


function makeSqlInsert(data, tableName, skipCols = []) {
  const columns = Object.keys(data[0]).join(",");
  const values = data.map(row => {
    return "(" + Object.values(row).map(val => {
      if (skipCols.includes(val) || val === null || val === 'None' || val === 'NaT' || val === 'nan' || val === '_' || val === '') {
        return 'NULL';
      }
      return `'${String(val).replace(/'/g, "").replace(/"/g, "'")}'`;
    }).join(",") + ")";
  }).join(",");

  return `INSERT INTO ${tableName} (${columns}) VALUES ${values};`;
}

// Generate Fake data
const data = Array.from({ length: 10 }, () => ({
  id: crypto.randomUUID(),
  text: faker.lorem.paragraph(),
  date_added: new Date().toISOString().slice(0, 19).replace("T", " ")
}));

const sqlInsert = makeSqlInsert(data, TARGET_TABLE);

// console.log(sqlInsert);


async function pollImport(bookmark) {
  const payload = {
    action: "poll",
    current_bookmark: bookmark
  };

  while (true) {
    const pollResponse = await fetch(D1_URL, {
      method: "POST",
      headers,
      body: JSON.stringify(payload)
    });

    const result = await pollResponse.json();
    console.log("Poll Response:", result.result);

    const { success, error } = result.result;

    if (success || (!success && error === "Not currently importing anything.")) {
      break;
    }

    await new Promise(resolve => setTimeout(resolve, 1000));
  }
}

// Upload to D1
async function uploadToD1() {
  // Init upload
  // Replace the existing hash generation
  const hash_str = crypto.createHash('md5').update(sqlInsert).digest('hex');

  const initResponse = await fetch(D1_URL, {
    method: "POST",
    headers,
    body: JSON.stringify({
      action: "init",
      etag: hash_str
    })
  });

  const uploadData = await initResponse.json();
  const uploadUrl = uploadData.result.upload_url;
  const filename = uploadData.result.filename;

  // Upload to R2
  const r2Response = await fetch(uploadUrl, {
    method: "PUT",
    body: sqlInsert
  });

  const r2Etag = r2Response.headers.get("ETag").replace(/"/g, "");

  console.log('Local hash:', hash_str);
  console.log('R2 hash:', r2Etag);

  // Verify etag
  if (r2Etag !== hash_str) {
    throw new Error("ETag mismatch");
  }

  // Start ingestion
  const ingestResponse = await fetch(D1_URL, {
    method: "POST",
    headers,
    body: JSON.stringify({
      action: "ingest",
      etag: hash_str,
      filename
    })
  });

  const ingestData = await ingestResponse.json();
  console.log("Ingestion Response:", ingestData);

  // Start polling
  await pollImport(ingestData.result.at_bookmark);

  return "Import completed successfully";
}

console.log(await uploadToD1());

// init
// ingest -> filename
// poll

async function runImport() {
  const result = await uploadToD1();
  console.log(result);
}

// 3. Run everything
runImport().catch(console.error);
```

## 8. Run the code

Run your code.

```sh
node index.js
```

## Summary

By completing this tutorial, you have

1. Created an API token.
2. Created a target database and table.
3. Generated example data.
4. Prepared the example data into a SQL command.
5. Imported your example data into the D1 target table using REST API.

